{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30715,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install accelerate -U > /dev/null"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers > /dev/null"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q jiwer > /dev/null"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "cer_metric = load_metric(\"cer\")\n",
    "wer_metric = load_metric(\"wer\")"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!kaggle datasets download -d constantinwerner/cyrillic-handwriting-dataset"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!unzip -o /kaggle/working/cyrillic-handwriting-dataset.zip -d /kaggle/working/cyrillic-handwriting-dataset > /dev/null"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!rm -rf /kaggle/working/cyrillic-handwriting-dataset.zip"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# basic random seed\n",
    "import os \n",
    "import random\n",
    "import numpy as np \n",
    "\n",
    "DEFAULT_RANDOM_SEED = 2021\n",
    "def seedBasic(seed=DEFAULT_RANDOM_SEED):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "# torch random seed\n",
    "import torch\n",
    "def seedTorch(seed=DEFAULT_RANDOM_SEED):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "      \n",
    "# basic + torch \n",
    "def seedEverything(seed=DEFAULT_RANDOM_SEED):\n",
    "    seedBasic(seed)\n",
    "    seedTorch(seed)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def seed(my_seed=42):\n",
    "    seedEverything(my_seed)\n",
    "    global g\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(my_seed)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "seed(42)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv('/kaggle/working/cyrillic-handwriting-dataset/train.tsv', delimiter='\\t', header=None)\n",
    "train_df.rename(columns={0: \"file_name\", 1: \"text\"}, inplace=True)\n",
    "\n",
    "test_df = pd.read_csv('/kaggle/working/cyrillic-handwriting-dataset/test.tsv', delimiter='\\t', header=None)\n",
    "test_df.rename(columns={0: \"file_name\", 1: \"text\"}, inplace=True)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_df.head()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_df.head()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Erosion(ImageOnlyTransform):\n",
    "    def __init__(self, safe_db_lists=[], prob=0.5) -> None:\n",
    "        super(Erosion, self).__init__()\n",
    "        self.safe_db_lists = safe_db_lists\n",
    "        self.prob = prob\n",
    "\n",
    "    def apply(self, img, copy=True, **params):\n",
    "        if np.random.uniform(0, 1) > self.prob:\n",
    "            return img\n",
    "        if copy:\n",
    "            img = img.copy()\n",
    "\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (3, 3))\n",
    "        img = cv2.erode(img, kernel, iterations=1)\n",
    "\n",
    "        return img\n",
    "\n",
    "class Dilation(ImageOnlyTransform):\n",
    "    def __init__(self, safe_db_lists=[], prob=0.5) -> None:\n",
    "        super(Dilation, self).__init__()\n",
    "        self.safe_db_lists = safe_db_lists\n",
    "        self.prob = prob\n",
    "\n",
    "    def apply(self, img, copy=True, **params):\n",
    "        if np.random.uniform(0, 1) > self.prob:\n",
    "            return img\n",
    "        if copy:\n",
    "            img = img.copy()\n",
    "\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (3, 3))\n",
    "        img = cv2.dilate(img, kernel, iterations=1)\n",
    "\n",
    "        return img"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "transform = A.Compose([\n",
    "    A.OneOf([\n",
    "        Erosion(),\n",
    "        Dilation()\n",
    "    ], p=.3),\n",
    "\n",
    "    A.OneOf([\n",
    "        A.GaussNoise(),\n",
    "        A.ISONoise(),\n",
    "        A.MultiplicativeNoise(),\n",
    "        A.ImageCompression(),\n",
    "        A.Sharpen()\n",
    "    ], p=.3),\n",
    "\n",
    "    A.ShiftScaleRotate(p=.3, shift_limit=(-0.0625, 0.0625), scale_limit=(-0.2, 0.05), rotate_limit=(-10, 10),\n",
    "                       border_mode=0, value=(199, 185, 182)),\n",
    "\n",
    "    A.OneOf([\n",
    "        A.ElasticTransform(alpha=.5, sigma=10, alpha_affine=.75, border_mode=0, value=(199, 185, 182)),\n",
    "        A.OpticalDistortion(distort_limit=(-0.3, 1.5), shift_limit=(-0.5, 0.5), border_mode=0, value=(199, 185, 182)),\n",
    "        A.GridDistortion(distort_limit=(-0.2, 0.2), border_mode=0, value=(199, 185, 182)),\n",
    "    ], p=.3),\n",
    "\n",
    "    A.OneOf([\n",
    "        A.ChannelDropout(),\n",
    "        A.ChannelShuffle(),\n",
    "        A.Posterize(),\n",
    "        A.RGBShift(),\n",
    "        A.ToGray(),\n",
    "        A.ToSepia()\n",
    "    ], p=.3),\n",
    "\n",
    "    A.OneOf([\n",
    "        A.CLAHE(clip_limit=2),\n",
    "        A.RandomBrightnessContrast(),\n",
    "        A.HueSaturationValue(),\n",
    "    ], p=.3),\n",
    "\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(blur_limit=3),\n",
    "        A.Blur(blur_limit=3),\n",
    "    ], p=.3),\n",
    "], p=0.6)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import TrOCRProcessor\n",
    "\n",
    "# microsoft/trocr-base-handwritten\n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class CyrillicDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, transform=None, max_target_length=128):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get file name + text\n",
    "        file_name = self.df['file_name'][idx]\n",
    "        text = self.df['text'][idx]\n",
    "        \n",
    "        if not isinstance(text, str):\n",
    "            text = ''\n",
    "        \n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        image = Image.open(f\"{self.root_dir}/{file_name}\").convert(\"RGB\")\n",
    "            \n",
    "#         image = cv2.imread(f\"{self.root_dir}/{file_name}\")\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = transform(image=image)['image']\n",
    "\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        # add labels (input_ids) by encoding the text\n",
    "        labels = self.processor.tokenizer(text,\n",
    "                                          padding=\"max_length\",\n",
    "                                          max_length=self.max_target_length).input_ids\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset_root_dir = '/kaggle/working/cyrillic-handwriting-dataset/train'\n",
    "eval_dataset_root_dir = '/kaggle/working/cyrillic-handwriting-dataset/test'\n",
    "\n",
    "train_dataset = CyrillicDataset(root_dir=train_dataset_root_dir,\n",
    "                                df=train_df,\n",
    "                                processor=processor)\n",
    "eval_dataset = CyrillicDataset(root_dir=eval_dataset_root_dir,\n",
    "                               df=test_df,\n",
    "                               processor=processor)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import ConcatDataset, Subset\n",
    "\n",
    "train_subset_size = len(train_dataset) // 8\n",
    "train_subset_indices = torch.randperm(len(train_dataset))[:train_subset_size].tolist()\n",
    "train_dataset = Subset(train_dataset, train_subset_indices)\n",
    "train_subset_indices[:10]"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "len(train_dataset)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def visualize(image):\n",
    "    plt.figure(figsize=(2, 2))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "path = f\"{train_dataset_root_dir}/{train_df['file_name'][0]}\"\n",
    "print(f\"{path=}\")\n",
    "image = cv2.imread(path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "visualize(image)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# image background color\n",
    "\n",
    "for _ in range(2):\n",
    "    random_number = random.randint(1, len(os.listdir(train_dataset_root_dir)))\n",
    "    image = cv2.imread(f\"{train_dataset_root_dir}/{train_df['file_name'][random_number]}\")\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    if transform is not None:\n",
    "        augmented_image = transform(image=image)['image']\n",
    "    visualize(augmented_image)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "encoding = train_dataset[0]\n",
    "for k, v in encoding.items():\n",
    "  print(k, v.shape)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "image = Image.open(f\"{train_dataset_root_dir}/{train_df['file_name'][0]}\").convert(\"RGB\")\n",
    "image"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "labels = encoding['labels']\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor.decode(labels, skip_special_tokens=True)\n",
    "print(label_str)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import VisionEncoderDecoderModel\n",
    "\n",
    "# microsoft/trocr-base-handwritten\n",
    "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten').to(device)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from pynvml import *\n",
    "\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print_gpu_utilization()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    num_train_epochs=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    fp16=True,\n",
    "    output_dir=\"/kaggle/working/seq2seq_model_handwritten\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    report_to=\"tensorboard\",\n",
    "    save_strategy=\"epoch\",\n",
    ")"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def accuracy(labels, pred_labels):\n",
    "    \"\"\"\n",
    "    Calculate accuracy of the model.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    assert len(labels) == len(pred_labels)\n",
    "    return np.sum(np.compare_chararrays(labels, pred_labels, \"==\", False)) / len(labels)\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "#     print(f\"{pred.predictions=}\")\n",
    "#     print(f\"{pred.label_ids=}\")\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions[0]\n",
    "    \n",
    "#     pred_ids[pred_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    \n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    \n",
    "#     print(f\"{pred_str=}\\n{'-' * 20}\\n{label_str=}\")\n",
    "#     print()\n",
    "          \n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    acc = accuracy(pred_str, label_str)\n",
    "\n",
    "    return {\"cer\": cer, \"wer\": wer, \"acc\": acc}\n",
    "\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Original Trainer may have a memory leak. \n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    pred_ids = torch.argmax(logits[0], dim=-1)\n",
    "    return pred_ids, labels"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=default_data_collator,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    ")\n",
    "trainer.train()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!zip -r \"/kaggle/working/seq2seq_model_handwritten.zip\" \"/kaggle/working/seq2seq_model_handwritten\""
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(r'seq2seq_model_handwritten.zip')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "\n",
    "def batch(iterable, batch_size):\n",
    "    \"\"\"Yield successive batches of given size from the iterable.\"\"\"\n",
    "    for i in range(0, len(iterable), batch_size):\n",
    "        yield iterable[i:i + batch_size]\n",
    "        \n",
    "\n",
    "def flatten(matrix):\n",
    "    return list(chain.from_iterable(matrix))\n",
    "\n",
    "def read_and_show(image_path):\n",
    "    \"\"\"\n",
    "    :param image_path: String, path to the input image.\n",
    " \n",
    " \n",
    "    Returns:\n",
    "        image: PIL Image.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    return image\n",
    "\n",
    "\n",
    "def ocr(images, processor, model):\n",
    "    \"\"\"\n",
    "    :param image: PIL Image.\n",
    "    :param processor: Huggingface OCR processor.\n",
    "    :param model: Huggingface OCR model.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        generated_text: the OCR'd text string.\n",
    "    \"\"\"\n",
    "    # We can directly perform OCR on cropped images.\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    pixel_values = processor(images, return_tensors='pt').pixel_values.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad(), torch.inference_mode():\n",
    "        generated_ids = model.to(device).generate(pixel_values)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return generated_text"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def eval_new_data(data_path, num_samples=50):\n",
    "    image_paths = glob.glob(data_path)\n",
    "    \n",
    "    acc_avg = AverageMeter()\n",
    "    wer_avg = AverageMeter()\n",
    "    cer_avg = AverageMeter()\n",
    "    \n",
    "    for i, image_paths in tqdm(enumerate(batch(image_paths, num_samples)), total=(len(image_paths) // num_samples)):\n",
    "        images = list(map(read_and_show, image_paths))\n",
    "        \n",
    "        def get_label(image_path):\n",
    "            image_name = image_path.split('/')[-1]\n",
    "            label = test_df.loc[test_df['file_name'] == image_name]['text'].values[0]\n",
    "            return label\n",
    "        \n",
    "        labels = list(map(get_label, image_paths))\n",
    "        print(labels[:10])\n",
    "        \n",
    "        start = time.time()\n",
    "        texts = ocr(images, processor, model)\n",
    "        print(texts[:10])\n",
    "        end = time.time()\n",
    "        \n",
    "        print(f\"Time: {end - start}\")\n",
    "        \n",
    "        cer = cer_metric.compute(predictions=texts, references=labels)\n",
    "        wer = wer_metric.compute(predictions=texts, references=labels)\n",
    "        print(f\"{wer=} {cer=}\")\n",
    "        \n",
    "        wer_avg.update(wer, 1)\n",
    "        cer_avg.update(cer, 1)\n",
    "    \n",
    "    return wer_avg, cer_avg\n",
    "\n",
    "wer_avg, cer_avg = eval_new_data(\n",
    "    data_path=os.path.join('/kaggle/working/cyrillic-handwriting-dataset', 'test', '*'),\n",
    "    num_samples=100\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
